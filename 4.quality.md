# Quality Protocol – Testing & Validation

This protocol defines how to test and validate Big Picture features, using AI to help design tests while keeping humans in control of correctness and game feel.

---

## Plan–Do–Check–Act loop

### 1. Plan

Before writing or changing code:

- **Align with spec**
  - Re-read the relevant section of `spec/<feature>.md` (for Big Picture: `spec/big-picture.md`) and list the acceptance criteria that this change should satisfy.
  - Note any cross-feature dependencies (e.g., lobby behavior that affects game state, or image generation affecting evaluation).

- **Identify edge cases and failure modes**
  - For multiplayer and game state:
    - Min and max player counts (2 vs 8 players).
    - Players leaving or disconnecting mid-lobby or mid-game.
    - Duplicate nicknames, invalid room codes, stale sessions.
  - For AI image generation:
    - Timeouts or HTTP errors from the image model.
    - Invalid or missing image IDs.
  - For evaluation:
    - Missing or incomplete action history.
    - Goal image not available or mismatched.

Capture key edge cases in a short list at the top of the test file or in `tests/<feature>-notes.md`.

---

### 2. Do

Use AI as a **test generator**, but keep the architecture and correctness under human control.

- **Ask for tests first (or in parallel)**
  - For new modules or functions, prompt AI to draft tests before the main implementation when possible to encourage spec-driven design.
  - Example prompt:
    - “Given the lobby behavior in spec/big-picture.md (room creation, join/leave/rejoin, 2–8 players), propose Rust unit tests for Room and GameState that cover happy path and edge cases. Use the existing module layout from CONTEXT.md.”

- **Test types**
  - **Unit tests**
    - For pure domain logic: `Room`, `GameState`, `PlayerAction`, evaluation functions, etc.
    - Example: joining/leaving players, enforcing capacity, advancing turns and rounds.
  - **Integration tests**
    - For API / session-manager boundaries (if separate backend exists).
    - For Godot + Rust integration where possible (e.g., a small harness that asserts correct scene transitions).
  - **Property / scenario tests (if added)**
    - For state machines: e.g., “starting from Lobby, a sequence of valid events must never produce an invalid state (like negative player counts or unknown current player).”

- **Test file layout (suggested)**
  - `tests/lobby.rs` – room creation, join/leave, capacity, rejoin.
  - `tests/game_flow.rs` – starting game, turns, rounds, finished state.
  - `tests/image_adapter.rs` – behavior when image generation succeeds/fails (mocked).
  - `tests/evaluation.rs` – outcome classification (Success/Close/Fail) given sample histories.

---

### 3. Check

After tests are written and implementation is in place:

- **Run tests locally**
  - `cargo test` for Rust domain, backend, and integration tests.
  - Any Godot-specific test harness or headless mode you adopt (documented in `CONTEXT.md`).

- **Manual inspection of risky areas**
  - **Security & privacy**
    - Ensure no sensitive data (e.g., model keys, player IPs, auth tokens) is logged.
    - Ensure input validation for room codes, nicknames, and any external payloads.
  - **Concurrency & state**
    - For session manager, verify that concurrent joins/leaves cannot corrupt state (consider locks, channels, or actor-like patterns).
    - For image generation, ensure concurrent requests don’t mix up room IDs or image IDs.
  - **I/O & external services**
    - Check timeout handling, retries, and error messages for the image generator.
    - Ensure errors from Godot/Rust boundary are handled gracefully and don’t crash the game.

- **Specification alignment**
  - For each acceptance criterion in `spec/big-picture.md`, confirm:
    - At least one test explicitly covers it, or
    - It is intentionally manual (e.g., pure UX) and documented as such.

---

### 4. Act

When tests fail or code quality issues are found:

- **Focused fixes**
  - Ask AI for changes **limited to the failing area**:
    - “Here is the failing test and function. Propose a minimal fix to pass this test without changing public APIs or unrelated modules.”
  - Reject any suggestion that alters unrelated modules or introduces architectural changes without updating the spec/context.

- **Update tests and spec**
  - If a bug reveals a missing case in the spec:
    - Update `spec/big-picture.md` to include that scenario.
    - Add or update tests to lock in the new behavior.
  - If a test was unclear or brittle:
    - Refine test names, assertions, and comments so they clearly encode the intended behavior, not incidental implementation details.

- **Regression discipline**
  - Keep failing tests around as permanent regression tests.
  - When a bug is fixed, ensure:
    - There is at least one test that would fail without the fix and passes with it.

---

## AI testing prompts

Use short, explicit prompts that bind AI to your spec and context:

- **Unit tests for a function or type**
  - “Given these definitions of Room and GameState (paste code) and the lobby behavior in spec/big-picture.md (paste relevant section), generate Rust unit tests that cover normal joins, full room rejection, leave, and rejoin cases. Focus on edge cases.”

- **Integration tests for a flow**
  - “Given the ‘All is in!’ behavior in spec/big-picture.md, write integration tests for the session manager API that cover: starting a game with 2–8 players, rejecting new joins after start, and ensuring the game state is initialized with a goal and starting image (using test stubs).”

- **Property / scenario tests**
  - “Treat the game flow as a state machine with Lobby, InGame, and Finished. Propose property-style tests or structured scenarios that ensure invalid sequences (e.g., start with 0 players, take a turn in Lobby) are rejected.”

---

## Code review checklist (human)

Use this checklist during review for every non-trivial change:

- **Correctness vs spec**
  - Does the behavior match `spec/big-picture.md` for this feature?
  - Are all relevant acceptance criteria covered by tests or explicitly marked as manual?

- **Consistency with CONTEXT**
  - Does the code follow architectural boundaries and conventions from `CONTEXT.md` (module layout, naming, error handling, logging)?
  - Does it respect project constraints (Rust only, Godot + Rust bindings, no JS)?

- **Security & privacy**
  - Are inputs validated (room codes, nicknames, client-supplied data)?
  - Are error messages free of sensitive details (no secrets, internal IDs that shouldn’t leak)?
  - Are any tokens or keys for image generation stored and used safely?

- **Performance & scalability**
  - Any obvious performance pitfalls in hot paths (e.g., per-frame allocations, blocking I/O on the main loop)?
  - Is the image-generation adapter designed so that slow calls do not freeze the main game loop?

- **Robustness & resilience**
  - Does the code handle timeouts, network errors, and partial failures gracefully?
  - Are default/fallback behaviors sensible (e.g., using placeholder images if the model fails in a demo build)?

- **Test coverage and quality**
  - Do tests cover:
    - Normal flow and edge cases?
    - At least one failure path or error handling path?
  - Are tests readable and stable (not tightly coupled to internal implementation details)?

- **Clarity & maintainability**
  - Is the intent of each function/module clear from its name and type signatures?
  - Are comments reserved for non-obvious invariants and decisions, not restating what the code plainly does?

By following this Quality Protocol, Big Picture remains testable, debuggable, and aligned with its specs as features evolve, while using AI to accelerate test design without sacrificing human judgment.
